"""
Pipeline completo: Scraping + Embeddings + Cosmos DB
Processa artigos, gera embeddings com Google Gemini e salva no Cosmos DB
"""

import asyncio
import json
import os
from typing import List, Dict, Any
import pandas as pd
from dotenv import load_dotenv
import google.generativeai as genai
from azure.cosmos import CosmosClient, PartitionKey

# Imports do projeto
from extract.extractor import extract_url
from extract.sectionizer import sectionize_text

# Carregar .env
load_dotenv()

# Configura√ß√µes
CSV_FILE_PATH = 'shared/SB_publication_PMC.csv'
OUTPUT_JSONL_PATH = 'shared/extracted_data_with_embeddings.jsonl'

# Google Gemini
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_EMBED_MODEL = os.getenv('GOOGLE_EMBED_MODEL', 'models/text-embedding-004')

# Cosmos DB
COSMOS_ENDPOINT = os.getenv('COSMOS_ENDPOINT')
COSMOS_KEY = os.getenv('COSMOS_KEY')
COSMOS_DATABASE = os.getenv('COSMOS_DATABASE', 'cosmos27818-db')
COSMOS_CONTAINER = os.getenv('COSMOS_CONTAINER', 'cosmos27818-container')

# Configurar Gemini
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
    print(f"‚úÖ Google Gemini configurado: {GOOGLE_EMBED_MODEL}")
else:
    print("‚ö†Ô∏è  GOOGLE_API_KEY n√£o configurado - embeddings n√£o ser√£o gerados")


def generate_embedding(text: str) -> List[float]:
    """
    Gera embedding usando Google Gemini.
    
    Args:
        text: Texto para gerar embedding
        
    Returns:
        Lista de floats representando o embedding (768 dimens√µes)
    """
    if not GOOGLE_API_KEY:
        return []
    
    try:
        # Limitar tamanho do texto (Gemini tem limite)
        max_chars = 10000
        if len(text) > max_chars:
            text = text[:max_chars]
        
        result = genai.embed_content(
            model=GOOGLE_EMBED_MODEL,
            content=text,
            task_type="retrieval_document"
        )
        
        return result['embedding']
    
    except Exception as e:
        print(f"‚ùå Erro ao gerar embedding: {e}")
        return []


async def process_article(row: pd.Series, idx: int, total: int) -> Dict[str, Any]:
    """
    Processa um artigo: extrai conte√∫do, secciona e gera embedding.
    
    Args:
        row: Linha do DataFrame com dados do artigo
        idx: √çndice atual
        total: Total de artigos
        
    Returns:
        Dicion√°rio com dados do artigo + embedding
    """
    url = row.get('URL') or row.get('url') or row.get('Link')
    title = row.get('Title') or row.get('title', 'Sem t√≠tulo')
    
    print(f"\n[{idx+1}/{total}] üìÑ Processando: {title[:60]}...")
    print(f"   URL: {url}")
    
    if not url or pd.isna(url):
        print("   ‚ö†Ô∏è  Sem URL, pulando...")
        return None
    
    try:
        # 1. Extrair conte√∫do
        print("   üîç Extraindo conte√∫do...")
        full_text, source_type = await extract_url(url)
        
        if not full_text or len(full_text) < 50:
            print(f"   ‚ùå Conte√∫do muito curto ou vazio")
            return None
        
        print(f"   ‚úÖ Extra√≠do: {len(full_text)} caracteres (tipo: {source_type})")
        
        # 2. Seccionar texto
        print("   üìë Seccionando texto...")
        sections = sectionize_text(full_text)
        
        # Extrair abstract e conte√∫do
        abstract = sections.get('abstract', '')
        if not abstract:
            # Pegar primeiros par√°grafos como abstract
            paragraphs = full_text.split('\n\n')
            abstract = paragraphs[0] if paragraphs else ''
        
        content = sections.get('introduction', '') + '\n\n' + sections.get('methods', '')
        if not content.strip():
            content = full_text
        
        # 3. Preparar texto para embedding (combina√ß√£o otimizada)
        embedding_text = f"{title}\n\n{abstract}\n\n{content[:2000]}"
        
        print(f"   üßÆ Gerando embedding...")
        embedding = generate_embedding(embedding_text)
        
        if not embedding:
            print("   ‚ö†Ô∏è  Embedding n√£o gerado")
        else:
            print(f"   ‚úÖ Embedding gerado: {len(embedding)} dimens√µes")
        
        # 4. Extrair keywords do texto
        keywords = []
        common_keywords = [
            'space', 'radiation', 'astronaut', 'microgravity', 'ISS',
            'cosmic', 'solar', 'mars', 'moon', 'satellite', 'orbit',
            'NASA', 'ESA', 'spacecraft', 'mission'
        ]
        
        text_lower = full_text.lower()
        for keyword in common_keywords:
            if keyword.lower() in text_lower:
                keywords.append(keyword)
        
        # 5. Preparar documento
        doc_id = f"article-{idx+1}"
        document = {
            'id': doc_id,
            'pk': doc_id,  # Partition key
            'doc_id': doc_id,
            'title': title,
            'abstract': abstract[:1000] if abstract else '',  # Limitar tamanho
            'content': content[:5000] if content else '',  # Limitar tamanho
            'full_text': full_text[:10000],  # Texto completo limitado
            'url': url,
            'source_type': source_type,
            'keywords': keywords,
            'sections': list(sections.keys()),
            'char_count': len(full_text),
            'has_embedding': len(embedding) > 0
        }
        
        # Adicionar embedding se gerado
        if embedding:
            document['embedding'] = embedding
        
        print(f"   ‚úÖ Documento preparado (id: {doc_id})")
        return document
    
    except Exception as e:
        print(f"   ‚ùå Erro ao processar: {e}")
        return None


async def main():
    """Fun√ß√£o principal do pipeline."""
    print("=" * 80)
    print("üöÄ PIPELINE: SCRAPING + EMBEDDINGS + COSMOS DB")
    print("=" * 80)
    print()
    
    # Verificar configura√ß√µes
    if not GOOGLE_API_KEY:
        print("‚ö†Ô∏è  AVISO: GOOGLE_API_KEY n√£o configurado")
        print("   Embeddings N√ÉO ser√£o gerados!")
        print()
    
    # Conectar ao Cosmos DB (se configurado)
    cosmos_client = None
    container = None
    
    if COSMOS_ENDPOINT and COSMOS_KEY:
        try:
            print(f"üìä Conectando ao Cosmos DB...")
            cosmos_client = CosmosClient(COSMOS_ENDPOINT, COSMOS_KEY)
            database = cosmos_client.get_database_client(COSMOS_DATABASE)
            container = database.get_container_client(COSMOS_CONTAINER)
            print(f"‚úÖ Conectado: {COSMOS_DATABASE}/{COSMOS_CONTAINER}")
            print()
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao conectar Cosmos DB: {e}")
            print("   Documentos ser√£o salvos apenas em JSONL")
            print()
    else:
        print("‚ö†Ô∏è  Cosmos DB n√£o configurado")
        print("   Documentos ser√£o salvos apenas em JSONL")
        print()
    
    # Ler CSV
    print(f"üìÇ Lendo CSV: {CSV_FILE_PATH}")
    try:
        df = pd.read_csv(CSV_FILE_PATH)
        total = len(df)
        print(f"‚úÖ {total} artigos encontrados")
        print()
    except FileNotFoundError:
        print(f"‚ùå Arquivo n√£o encontrado: {CSV_FILE_PATH}")
        return
    
    # Processar artigos
    documents = []
    
    # Processar TODOS os artigos (remover limite para processar tudo)
    # df = df.head(3)  # Comentado - processar todos
    
    for idx, row in df.iterrows():
        document = await process_article(row, idx, total)
        
        if document:
            documents.append(document)
            
            # Salvar no Cosmos DB se conectado
            if container:
                try:
                    container.upsert_item(document)
                    print(f"   üíæ Salvo no Cosmos DB")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Erro ao salvar no Cosmos: {e}")
        
        # Delay para n√£o sobrecarregar APIs
        await asyncio.sleep(1)
    
    # Salvar JSONL
    print()
    print(f"üíæ Salvando resultados em: {OUTPUT_JSONL_PATH}")
    
    with open(OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f:
        for doc in documents:
            # Remover embedding do JSONL (muito grande)
            doc_copy = doc.copy()
            if 'embedding' in doc_copy:
                doc_copy['embedding'] = f"[{len(doc['embedding'])} dimensions]"
            f.write(json.dumps(doc_copy, ensure_ascii=False) + '\n')
    
    print(f"‚úÖ {len(documents)} documentos salvos")
    
    # Estat√≠sticas
    print()
    print("=" * 80)
    print("üìä ESTAT√çSTICAS")
    print("=" * 80)
    print(f"Total processado:       {len(documents)}/{total}")
    print(f"Com embeddings:         {sum(1 for d in documents if d.get('has_embedding'))}")
    print(f"Salvos no Cosmos DB:    {len(documents) if container else 0}")
    print(f"Salvos em JSONL:        {len(documents)}")
    
    if documents:
        avg_chars = sum(d['char_count'] for d in documents) / len(documents)
        print(f"M√©dia de caracteres:    {avg_chars:.0f}")
    
    print()
    print("üéâ Pipeline conclu√≠do!")


if __name__ == "__main__":
    asyncio.run(main())
